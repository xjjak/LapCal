#+title: Data Collection and Preprocessing

#+property: header-args:jupyter-python :session *jupyter* :eval no-export
#+OPTIONS: ^:nil h:6

This document describes all software related to data collection, when possible in a literate programming style.

Except for the [[*A Brief Description of Our Data][first chapter]] -- a broad descrpition of how our data will look for all datasets -- and the [[*The Data Collection Process][second chapter]] -- a description of the general collection process --, the chapters are organized by dataset type. Each dataset type chapter includes all relevant
- descriptions and design choices,
- software to collect the relevant data, and
- processing capabilities to convert raw data into refined datasets.

Notably, the dataset types mostly vary by how we collect the [[*Labels][labels]].

* Contents                                                         :noexport:
:PROPERTIES:
:TOC:      :include all :ignore (this)
:END:

# TOC automattically generated by [[https://github.com/alphapapa/org-make-toc]]
# NOTE: These links will *only* work on github.
:CONTENTS:
- [[#a-brief-description-of-our-data][A Brief Description of Our Data]]
  - [[#features][Features]]
  - [[#labels][Labels]]
- [[#the-data-collection-process][The Data Collection Process]]
- [[#collecting-data][Collecting Data]]
  - [[#capturing-keypresses-directly-from-the-keyboard][Capturing Keypresses Directly from the Keyboard]]
    - [[#description][Description]]
      - [[#raw-data][Raw Data]]
      - [[#dataset][Dataset]]
    - [[#keypresses-directly-from-the-keyboard][Keypresses Directly from the Keyboard]]
    - [[#rectify-errors][Rectify Errors]]
      - [[#find-errors][Find Errors]]
      - [[#fix-errors][Fix Errors]]
    - [[#a-look-at-the-data][A Look at the Data]]
    - [[#center-the-data][Center the Data]]
      - [[#a-visual-comparison-to-the-uncentered-data][A Visual Comparison to the Uncentered Data]]
      - [[#save-data][Save Data]]
    - [[#aggregate-features-and-labels][Aggregate Features and Labels]]
      - [[#features][Features]]
        - [[#change-based-features][Change-based Features]]
        - [[#history-based-features][History-based Features]]
      - [[#labels][Labels]]
    - [[#write-dataset][Write Dataset]]
:END:

* A Brief Description of Our Data
Before we start describing the software that is used in the data collection process, we first want to give a quick overview of what our data actually consist of and where it comes from.

The data we collect can broadly be divided into two categories, since we use supervised learning:
1. The [[*Features][features]] our model takes as input. These are necessary during training and deployment of the model.
2. The [[*Labels][labels]] our model produces as output. These are only necessary during training. During deployment the model itself will produce predictions for them.

** Features
The features are generated using the hardware developed in [[file:../Hardware][other parts of the project]] in a more or less straightforward. While the specifics might change, in general the data will always come from [[https://en.wikipedia.org/wiki/Inertial_measurement_unit][inertial measurement units]] on the fingers and the back of the hand. The IMUs we currently use (MPU6050) give us information about their current acceleration in all three spatial dimensions as well as their rotation, that is, yaw, pitch, and roll.

In some cases the data will be explicitly limited to a subset of all the available sensors, most prominently to just one sensor.

Further, we need to convert our sequential data into features that encapsulate temporal changes. For example, one option would be to concatenate a set number of successive data points into one feature, like a [[https://www.geeksforgeeks.org/window-sliding-technique/][sliding window]].

# TODO: why here and not in machine-learning.org?
#   avoid high volatility of datasets in machine-learning.org since
#   models can be sensitive to changes in dataset architecture.

** Labels
The labels we assign to our collected features, represent some kind of typing state. A typing state considers every physical key on a [[https://github.com/davidphilipbarr/Sweep][keyboard]] separately. But what a typing state associates to a key can differ depending on the use case:
- in some cases, we just collect whether keys were changed from an unpressed to a pressed state, (states: =UNCHANGED=, =PRESS=, =RELEASE=)
- in other cases, we collect the state of the keys themselves, that is, whether they are pressed or not. (states: =UNPRESSED=, =PRESSED=)

It is important to note that feature labels are /not necessarily unique/ and labelling should be done with caution to avoid issues arising from feature representation like the following: When using a sliding window and not careful when labelling, a single key press can cause a model recognizing press and release events fire twice. An example for a sliding window with size 5 might look like the following for two consecutive model inputs:
  #+begin_example
  [ 5  4  3  2  1 ] => Prediction: PRESS event
       ^            <- key press event
       
  [ 6  5  4  3  2 ] => Prediction: PRESS event
          ^         <- key press event
  #+end_example

Often, we only consider a subset of all possible key on our [[https://github.com/davidphilipbarr/Sweep][keyboard]], like for example only keys reachable by a certain finger or only one half of the keyboard. The reason for doing so is usually the assumption that the labels can be partitioned in such a way that different models can separately predict events independently from one another. Analogously, for some datasets we leave some features out since we assume that they don't affect the subset of labels we look at.

For some experiments, it could also be interesting to further simplify certain models, e.g. no differentiation between different keys pressed by a key, but these are *not accounted* for in this document since they are usually quick modifications that mostly serve experimental purposes.

* TODO The Data Collection Process
/This section is currently empty. In the future some general approaches to data collection might be listed here./

# TODO: python version + package management

* Collecting Data
This is the main part of this document. In this chapter, the [[*The Data Collection Process][general process]] for collecting data is adapted to our specific needs resulting in the actual datasets.

** TODO Capturing Keypresses Directly from the Keyboard
:PROPERTIES:
:header-args:jupyter-python: :async yes :session *jupyter-keyboard-direct*
:END:
*WARNING*: For the moment, this chapter /only describes data pertaining quick taps/, explicitly /excluding the handling of hold events/ and as such press and release events.

Currently the following datasets are recorded and processed:
- [[file:_datasets/2025-01-15_controlled-taps/]]
- [[file:_datasets/2025-01-20_monkeytype/]]

To LOAD a dataset execute the following code block and select the dataset you want to load.
#+name: load
#+header: :var dataset=(completing-read "Dataset: " (directory-files "_datasets/" nil directory-files-no-dot-files-regexp) nil t)
#+header: :var stage=(if current-prefix-arg (completing-read "Stage: " '("raw.csv" "formatted.csv" "fixed.csv" "centered.csv") nil t) "centered.csv")
#+begin_src jupyter-python :results silent
  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt
  from pathlib import Path
  data_dir = Path("_datasets/")
  data_path = data_dir / dataset
  print(f"Loading \"{data_path / stage}\"...\n")
  df = pd.read_csv(data_path / stage)
#+end_src

To PROCESS a new dataset, execute following code block or [[file:src/preprocess_dataset.py][the corresponding tangled script]] and input the dataset name.

# #+header: :comments both/link/org
#+header: :tangle "src/preprocess_dataset.py" :noweb no-export
#+begin_src jupyter-python :results silent
  #!/usr/bin/env python3

  from pathlib import Path
  import sys
  import os

  # NOTE: This program does only work on new datasets within the following dataset directory!

  data_dir = Path("_datasets/")
  assert os.path.isdir(data_dir), "Error: Cannot find dataset directory."

  if len(sys.argv) == 2:
      data_name = sys.argv[1]
  else:
      data_name = input("Dataset Name: ")

  assert data_name, "Error: No dataset given."
  data_path = data_dir / data_name
  assert os.path.isdir(data_path), "Error: Cannot find given dataset"

  # Import necessary libraries
  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt

  print("Finding incosistencies within the dataset...")
  <<find>>
  print("Fixing incosistencies...")
  <<fix>>
  print("Centering the data using an EMA...")
  <<center>>

  print("Preprocessing successful!")
#+end_src

*** Description
***** Raw Data
  File: [[file:_datasets/2025-01-15_controlled-taps.csv][2025-01-15_controlled-taps.csv]] (4.6Mb, 151471 lines, semicolon separated)
  
  Data was recorded by [[https://github.com/xjjak/][@xjjak]] on <2025-01-15 Wed> over a time of around 10 minutes using sensor #I [fn::#I refers to the physical label of the used sensor.] with the following calibration offsets ={-6018, 1394, 1385, 66, -86, 35}=. The recorded typing motions only contain *short taps* from the middle finger. During the recording, taps were performed in varying frequencies from different starting positions [fn::The position in which the finger rested before performing the tapping motion.] and the order in which the reachable keys were pressed also varies. To ensure consistency, the controller was reset with a brief waiting period beforehand.

  As a result of the recording methodology, there should only be three + one possible key states:
  - lower row, home row, upper row, as well as
  - no key pressed
  The data represents these in a binary format, i.e., the possible states are 0, 1, 2, and 4. Though, other values are technically possible, these are errors and should if possible (it often is) be fixed before proceeding.

  Similarly, only the data collected from one sensor was included. This especially means that there is no data from the sensor on the back of the hand.

  # TODO: format?
  
***** TODO Dataset
      
*** TODO Keypresses Directly from the Keyboard
# TODO: links to relevant code
# TODO: check for correctness (@xjjak)
To collect keypress data directly from the keyboard, we inject some firmware code into the keyboards firmware, that sends key events to an external controller (the project hardware?) via unused pins on the keyboards microcontroller.

*** Rectify Errors
Before we can recitify errors, we first need to be aware of what errors are. In this case, there are two kind of errors to consider -- we will only handle the latter:
1. *Sensor failure*: Sometimes the firmware fails to read data from the sensor or the data it reads doesn't really make any sense. But these kinds of errors are hard to sensibly rectify. Instead, it is often more reasonable to omit data around the errors or to let the machine learning algorithm handle the errors. For now, we will not handle them.
   # TODO: do we check for them? (ig prob should via histogram)
2. *Key event error*: This error is not a technical error per se but a result of typing inprecision. Sometimes multiple pressed keys are recognised, when only one was actually supposed to be pressed. This is the kind of error we can mostly fix because such a multikey press implies that the intended key is one of the recognised presses and the actual press is at the border of both of them.

The next step would be to find those errors in the raw data and handle them -- if appropriate.

**** Find Errors
First, we need to load the data. We do this using [[https://pandas.pydata.org/][pandas]].

#+header: :noweb-ref find
#+begin_src jupyter-python
  # Load the dataset using pandas
  df = pd.read_csv(data_path/'raw.csv', sep=';', dtype=np.float64)
  df.dtypes
#+end_src

#+RESULTS:
: kb_state    float64
: ax          float64
: ay          float64
: az          float64
: gx          float64
: gy          float64
: gz          float64
: dtype: object

Since =kb_state= -- unlike the other columns -- is not a float but an integer value, we reflect this in our data frame.

#+header: :noweb-ref find
#+begin_src jupyter-python
  df = df.astype({'kb_state': np.int64})
  df.dtypes
#+end_src

#+RESULTS:
: kb_state      int64
: ax          float64
: ay          float64
: az          float64
: gx          float64
: gy          float64
: gz          float64
: dtype: object

To avoid some common binary writing/reading problems at the start of the file, we discard the first data row. This is also the first intermediary result we save since that makes it easy to load and work with it using pandas.

#+header: :noweb-ref find
#+begin_src jupyter-python :results silent
  df.drop(index=df.index[0], inplace=True)
  df.reset_index(drop=True, inplace=True)
  df.to_csv(data_path/'formatted.csv', index=False)
#+end_src

The raw data consists of a list of readings. These however do not include enough context, to detect all erroneous readings and fix the errors. Instead of the readings themselves, we consider /taps/. As a tap, we understand a maximally long sequence of consecutive readings with nonzero keyboard states.

#+header: :noweb-ref find
#+begin_src jupyter-python :results silent
  # Get starting (inclusive) and ending (exclusive) index of reading
  # around given reading.
  def get_tap_around(reading_idx, df):
      # check if inside tap
      assert df.kb_state[reading_idx] != 0
      
      i = reading_idx
      while i > 0 and df.kb_state[i-1] != 0:
          i -= 1
      a = i
      while i < len(df) and df.kb_state[i] != 0:
          i += 1
      b = i
      
      return a, b
#+end_src

To check the taps for errors, we first need to extract all taps from the given data.

#+header: :noweb-ref find
#+begin_src jupyter-python :results silent
  idx = 0
  taps = list()
  while idx < len(df):
      if df.kb_state[idx] != 0:
          tap = get_tap_around(idx, df)
          taps.append(tap)
          idx = tap[1]
      else:
          idx += 1
#+end_src

Next, we identify the erroneous ones. As outlined in the [[*A Brief Description of Our Data][data description]], the only valid /keyboard states/ (first column in the raw data file) are 0, 1, 2, and 4 -- i.e., 000, 001, 010, and 100 in binary. Invalid keyboard states would consequently be 3 (011), 5 (101), 6 (110), and 7 (111). Thus, all taps that contain any invalid keyboard states are erroneous. A tap is also faulty when it contains multiple valid keyboard states, since one tap should also only hit one key. We write the following function to validate taps.

#+header: :noweb-ref find
#+begin_src jupyter-python :results silent
  valid_keyboard_states = [0, 1, 2, 4]
  def is_tap_valid(tap, df):
      return len(set(map(lambda i: df.kb_state[i], range(*tap)))) == 1 \
          and df.kb_state[tap[0]] in valid_keyboard_states
#+end_src

Using the function we can filter for invalid taps.

#+header: :noweb-ref find
#+begin_src jupyter-python :exports both
  taps_err = list(filter(lambda tap: not is_tap_valid(tap, df), taps))
  
  # Print overview of all erroneous taps
  print("Erroneous taps:")
  for tap in sorted(taps_err):
      start, end = tap
      states = set()
      for j in range(start, end):
          states.add(df.kb_state[j])
      print(f"  from {tap[0]:6d} to {tap[1]:6d} with states: {', '.join(map(str, states))}")
#+end_src

#+RESULTS:
: Erroneous taps:
:   from  23626 to  23639 with states: 2, 4, 6
:   from  50181 to  50205 with states: 2, 6
:   from  53461 to  53482 with states: 2, 6
:   from  62912 to  62925 with states: 2, 3
:   from 106847 to 106860 with states: 2, 6
:   from 126433 to 126451 with states: 2, 6
:   from 134939 to 134955 with states: 2, 6

**** Fix Errors
To fix these issues, we have to come up with a strategy for reassigning these multiple, possibly invalid states into one state per tap. The strategy we employ works the following way:
- When there is *at least one valid state*, we assign the first valid state to the tap.
- When there is *only one invalid state*, we assign the state to the tap that corresponds to the non-homerow key that is part of the invalid state.
- /Other situations are not covered for now, since we do not encounter them./

#+header: :noweb-ref fix
#+begin_src jupyter-python :results silent
  reassignments = dict()

  for tap in taps_err:
      start, end = tap
      contains_valid = False
      for i in range(start, end):
          if df.kb_state[i] in valid_keyboard_states:
              assignment = df.kb_state[i]
              contains_valid = True
              break
      if not contains_valid:
          if df.kb_state[start] & 1:
              assignment = 1
          elif df.kb_state[start] & 4:
              assignment = 4
          else:
              assignment = 2

      reassignments[tap] = assignment
#+end_src

Now we need to apply these reassignments. When a tap is assigned to a certain keyboard state, applying that assignment just means to set the keyboard state of every reading in the tap to the assigned state.

#+header: :noweb-ref fix
#+begin_src jupyter-python :results silent
  for tap, assignment in reassignments.items():
      start, end = tap
      for i in range(start, end):
          df.loc[0,"kb_state"] = assignment
#+end_src

Lastly, we write the data to an intermediary file.

#+header: :noweb-ref fix
#+begin_src jupyter-python :results silent
  df.to_csv(data_path/'fixed.csv', index=False)
#+end_src

*** A Look at the Data
# TODO: Sections from here on onwards should not depend on the previous section having run.
This section works with the data generated by the [[*Rectify Errors][previous section]].
#+begin_src jupyter-python
  df = pd.read_csv(data_path/'fixed.csv')
  df.dtypes
#+end_src

#+RESULTS:
: kb_state      int64
: ax          float64
: ay          float64
: az          float64
: gx          float64
: gy          float64
: gz          float64
: dtype: object

Before we continue to aggregate the data into feature vectors, we want to take a quick look at the data using [[https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html][histograms with matplotlib]].

First, we look at the the distribution of keyboard states.

#+header: :file diagrams/2025-01-15_controlled-taps_histogram-keyboard-states.png
#+begin_src jupyter-python :results output :exports both
  unique, counts = np.unique(df.kb_state, return_counts=True)
  plt.bar(unique, counts, label="kb_state")
  plt.legend(prop={'size': 10})
  plt.title('histogram of keyboard states')
#+end_src

#+RESULTS:
[[file:diagrams/2025-01-15_controlled-taps_histogram-keyboard-states.png]]

As expected, the majority of states are resting states and fortunately the other states seem equally frequent.

Next, we look at the acceleration data.

#+header: :file diagrams/2025-01-15_controlled-taps_histogram-acceleration.png
#+begin_src jupyter-python :results output :exports both
  labels = ['ax','ay','az']
  plt.hist(df[labels], bins=100, density=True, label=['ax','ay','az'], histtype='stepfilled')
  plt.legend(prop={'size': 10})
  plt.title('histogram of acceleration data')
#+end_src

#+RESULTS:
[[file:diagrams/2025-01-15_controlled-taps_histogram-acceleration.png]]

The first thing we notice is that the calibration does not seem to work that well but this is something we have come to expect since the absolute values seem to drift unpredictably over time. Apart from that, we also notice that =ax= and =az= seem to be distributed normally with little variation, unlike =ay= which is distributed much more broadly and seemingly also not in a normal distribution. This could indicate, that =ay= could play an important role detecting taps. We also, notably, don't see any significant amount noise which is good.

Lastly, we look at the rotation data.

#+header: :file diagrams/2025-01-15_controlled-taps_histogram-rotation.png
#+begin_src jupyter-python :results output :exports both
  labels = ['gx','gy','gz']
  plt.hist(df[labels], bins=100, density=True, label=labels, histtype='stepfilled')
  plt.legend(prop={'size': 10})
  plt.title('histogram of rotation data')
#+end_src

#+RESULTS:
[[file:diagrams/2025-01-15_controlled-taps_histogram-rotation.png]]

For the rotation data we also fortunately do not observe any significant amount of noise. All rotation axes seem to be normally distributed which we expect since any variation from typing on different keys should be observed equally frequent on both ends. Also, =gx= and =gz= are distributed slightly more broadly than =gy= which might indicate that these are the axes that the finger rotates around while typing.

*** Center the Data
This section also works with the data from the [[*Rectify Errors][first section]].
#+begin_src jupyter-python
  df = pd.read_csv(data_path/'fixed.csv')
  df.dtypes
#+end_src

#+RESULTS:
: kb_state      int64
: ax          float64
: ay          float64
: az          float64
: gx          float64
: gy          float64
: gz          float64
: dtype: object

As we saw in [[*A Look at the Data][A Look at the Data]], the sensor calibration is not reasonable reliable over the long term, drifts accumulate over time and calibrating the sensors every time you want to use the device is infeasible. Instead, we want to try dynamic calibration: The first approach that comes to mind is to keep some sort of average that favors more recent data, like a [[https://en.wikipedia.org/wiki/Moving_average][moving average]]. We want to try to use the [[https://en.wikipedia.org/wiki/Exponential_smoothing][exponantial moving average]].

#+header: :noweb-ref center
#+begin_src jupyter-python :results silent
  def center_moving_average(df, alpha=0.995):
      df = df.copy()
      df.iloc[:,1:] = df.iloc[:,1:] - df.iloc[:,1:].ewm(alpha=1-alpha).mean()
      return df

  df_center = center_moving_average(df, alpha=0.995)
#+end_src

**** A Visual Comparison to the Uncentered Data
We want the exponential moving average to approximate the real average of the data. The closer we get to that, the more similar the distribution should look to the uncentered distribution. Essentially, the moving average should only act as a global shift as much as possible.

#+name: compare_ema
#+begin_src jupyter-python :results output :exports code :var alpha=0.95 column="ay"
  df_tmp = center_moving_average(df, alpha=alpha)
  plt.hist(
      [df_tmp.loc[:,column], (df - df.mean()).loc[:,column]],
      bins=50,
      label=[column + " (EMA)", column + " (centered)"],
      histtype='step',
      linewidth=2
  )
  plt.legend(prop={'size': 10})
  plt.title(f'histogram of acceleration data (alpha={alpha})')
#+end_src

#+RESULTS: compare_ema
[[file:./.ob-jupyter/3807dfad30228b4644ad9c1409c6909162faaa7c.png]]

# TODO: does not render in github.

Quick comparison of different averaging weights with =column=2=:
- =alpha=0.5=
  #+call: compare_ema[:file diagrams/ema_centering/compare_0-5.png :exports results](alpha=0.5)

  #+RESULTS:
  [[file:diagrams/ema_centering/compare_0-5.png]]
  
- =alpha=0.8=
  #+call: compare_ema[:file diagrams/ema_centering/compare_0-8.png :exports results](alpha=0.8)

  #+RESULTS:
  [[file:diagrams/ema_centering/compare_0-8.png]]
  
- =alpha=0.9=
  #+call: compare_ema[:file diagrams/ema_centering/compare_0-9.png :exports results](alpha=0.9)

  #+RESULTS:
  [[file:diagrams/ema_centering/compare_0-9.png]]
  
- =alpha=0.95=
  #+call: compare_ema[:file diagrams/ema_centering/compare_0-95.png :exports results](alpha=0.95)

  #+RESULTS:
  [[file:diagrams/ema_centering/compare_0-95.png]]
  
- =alpha=0.99=
  #+call: compare_ema[:file diagrams/ema_centering/compare_0-99.png :exports results](alpha=0.99)

  #+RESULTS:
  [[file:diagrams/ema_centering/compare_0-99.png]]
  
- =alpha=0.995=
  #+call: compare_ema[:file diagrams/ema_centering/compare_0-995.png :exports results](alpha=0.995)

  #+RESULTS:
  [[file:diagrams/ema_centering/compare_0-995.png]]

Since choosing =alpha=1= would just shift the all the data by the first value -- which is not reliable as a measure to center the data -- =alpha=0.995= gets us most similar fit. This also means that every new reading gets about as much weight as it would if we were to center a second worth of readings, since we read about src_jupyter-python{len(df)//(10*60)} {{{results(=252=)}}} readings per second (assuming a recording time of 10 min).

Taking a closer look at all the sensor readings, we get the following comparison.

#+name: compare-center
#+header: :var alpha=0.995 start=15000 end=17000 column="ay"
#+begin_src jupyter-python :results output :exports code
  df_tmp = center_moving_average(df, alpha=alpha)
  fig, axs = plt.subplots(3, 1, sharex=True, height_ratios=(10,10,3))
  axs[0].plot(np.arange(start, end), df.loc[start:end-1,column], label=column)
  axs[0].plot(np.arange(start, end), (df - df_tmp).loc[start:end-1,column], label=column + " (EMA)")
  axs[0].plot([start, end], [df.mean()[column], df.mean()[column]], label=column+" (mean)")
  axs[1].plot(np.arange(start, end), df_tmp.loc[start:end-1,column], label=column+" (centered)")
  axs[2].step(
      np.arange(start, end),
      np.minimum(df.kb_state[start:end], 3),
      label="taps",
  )
  axs[2].set_ylim([0, 3.5])
  axs[2].set_yticks([0, 1, 2, 3])
  axs[0].legend(prop={'size': 7}, bbox_to_anchor=(1.0, 1.0))
  axs[1].legend(prop={'size': 7}, bbox_to_anchor=(1.0, 1.0))
  axs[2].legend(prop={'size': 7}, bbox_to_anchor=(1.0, 1.0))
#+end_src

#+RESULTS: compare-center
[[file:./.ob-jupyter/803ce476f8aee9e9a71d25308a3285cf38c60501.png]]

For the following comparison we set =alpha=0.995=, =start=15000=, and =end=17000=
- ~column="ax"~:
  #+call: compare-center[:file diagrams/ema_centering/compare-center_1.png :exports results](column="ax")

  #+RESULTS:
  [[file:diagrams/ema_centering/compare-center_1.png]]
  
- ~column="ay"~:
  #+call: compare-center[:file diagrams/ema_centering/compare-center_2.png :exports results](column="ay")

  #+RESULTS:
  [[file:diagrams/ema_centering/compare-center_2.png]]
   
- ~column="az"~:
  #+call: compare-center[:file diagrams/ema_centering/compare-center_3.png :exports results](column="az")

  #+RESULTS:
  [[file:diagrams/ema_centering/compare-center_3.png]]
   
- ~column="gx"~:
   #+call: compare-center[:file diagrams/ema_centering/compare-center_4.png :exports results](column="gx")

  #+RESULTS:
  [[file:diagrams/ema_centering/compare-center_4.png]]
   
- ~column="gy"~:
  #+call: compare-center[:file diagrams/ema_centering/compare-center_5.png :exports results](column="gy")

  #+RESULTS:
  [[file:diagrams/ema_centering/compare-center_5.png]]
   
- ~column="gz"~:
  #+call: compare-center[:file diagrams/ema_centering/compare-center_6.png :exports results](column="gz")

  #+RESULTS:
  [[file:diagrams/ema_centering/compare-center_6.png]]

We notice that the average is pretty reliable for the acceleration data, but for the rotation data we see more fluctuations in the average and it is not clear how that might affect learning.

# TODO: should we center rotations?

**** Save Data
The centered data is another intermediary step we want to save.

#+header: :noweb-ref center
#+begin_src jupyter-python :results silent
  df_center.to_csv(data_path/'centered.csv', index=False)
#+end_src

*** TODO Aggregate Features and Labels
# TODO(!): update to use pandas
**** TODO Features
Every feature needs to encompass information about the state of the sensor as well as its immediate history. This is necessary to enable the model to detect changes in the sensor readings since two resting positions cannot be reliably differentiated even if one is in the air and the other on the table.

To add history information to our features, we consider two kinds of additional features:
- the *change* of every sensor value compared to its predecessor; if more history is necessary one could also add the *change of the change* and so on. (a form of discrete numerical differentiation)
- the *history* itself, i.e., we just add the previous sensor reading to our feature vector. For more history we can just add more readings.
  
Both approaches have their own set of benefits and drawbacks. We list some here:
- Simpler models might work better on change than on history values.
- Normalization techniques might affect the information in change values since they are dependent on other values in the feature vector. This might not necessarily a problem though, since it might be enough to just compare them in relation to other change values.
  
The conclusion here seems to be, that change values are more suitable for simpler models, especially if they work well without normalization, and that history values are the better choice for more complex models, like neural networks, and strongly benefit from normalization and through their complexity can consider change values implicitly on their own. Thus, we will prepare both datasets for further experimentation.

***** Change-based Features
# TODO: degree as org variable?

Before we can create change-based features, we first need to decide the /degree/, i.e., how many levels of change we include.

#+begin_src jupyter-python :results silent
  N_DEGREE = 5
  N_SENSOR_DIMS = 6
#+end_src

Then, we can create our feature vectors. We assume that higher levels of change can be initialized with zeros for the first few features vectors which corresponds to the absence of sensor movement which should be compatible with the data collection methodology.
# Should the first N_DEGREE be included? Assuming no change seems somewhat reasonable

#+begin_src jupyter-python :results silent
  features_ct_change = np.zeros((len(data_ct), N_DEGREE * N_SENSOR_DIMS))
  for d in range(N_DEGREE):
      features_ct_change[d, 0:N_SENSOR_DIMS] = data_ct[d,1:]

  for i in range(len(data_ct)):
      for d in range(min(N_DEGREE, i+1)):
          if d == 0:
              features_ct_change[i, 0:N_SENSOR_DIMS] = data_ct[i,1:]
          else:
              previous = features_ct_change[i-1, (d-1)*N_SENSOR_DIMS:d*N_SENSOR_DIMS]
              current  = features_ct_change[i, (d-1)*N_SENSOR_DIMS:d*N_SENSOR_DIMS]
              features_ct_change[i, d*N_SENSOR_DIMS:(d+1)*N_SENSOR_DIMS] = current - previous
#+end_src

***** History-based Features
# TODO: degree as org variable?

Similarly to the change-based features, the history-based features also need a degree -- in this case the number of previous readings to include.

#+begin_src jupyter-python :results silent
  N_DEGREE = 5
  N_SENSOR_DIMS = 6
#+end_src

With that, we can create the feature vectors. This time we discard the first =N_DEGREE= of potential feature vectors. Alternatively, we could also duplicate the first reading =N_DEGREE= times to achieve a similar effect to what we did for [[*Change-based Features][change-based feature vectors]].

#+begin_src jupyter-python :results silent
  features_ct_history = np.zeros((len(data_ct)-N_DEGREE, N_DEGREE * N_SENSOR_DIMS))
  for d in range(N_DEGREE):
      features_ct_history[0, d*N_SENSOR_DIMS:(d+1)*N_SENSOR_DIMS] = data_ct[N_DEGREE-d-1,1:]

  for i in range(1, len(data_ct)-N_DEGREE):
      for d in range(N_DEGREE):
          if d == 0:
              features_ct_history[i, 0:N_SENSOR_DIMS] = data_ct[N_DEGREE+i,1:]
          else:
              previous = features_ct_history[i-1, (d-1)*N_SENSOR_DIMS:d*N_SENSOR_DIMS]
              features_ct_history[i, d*N_SENSOR_DIMS:(d+1)*N_SENSOR_DIMS] = previous
#+end_src

**** TODO Labels
*** TODO Write Dataset
